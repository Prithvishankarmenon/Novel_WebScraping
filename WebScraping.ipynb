{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84acbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "import time\n",
    "from tqdm.notebook import tqdm  # For Jupyter-friendly progress bar\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "BASE_URL = \"https://\"# Example base URL, replace with actual URL\n",
    "# For example, if the novel is hosted on \"https://example.com\", set BASE_URL to that.\n",
    "NOVEL_SLUG = \"__\"# Example slug, replace with actual slug\n",
    "# For example, if the novel's URL is \"https://example.com/novel-title\", set NOVEL_SLUG to \"novel-title\".\n",
    "NOVEL_URL = f\"{BASE_URL}/{NOVEL_SLUG}.html\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Create a retry-enabled session\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    "    raise_on_status=False\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Function to get chapter content\n",
    "def get_chapter_content(url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait up to 15 seconds for either div.chapter-c or div.chapter-content to load\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        content_div = None\n",
    "\n",
    "        # Try to wait for first div (chapter-c)\n",
    "        try:\n",
    "            content_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.chapter-c\")))\n",
    "        except:\n",
    "            # If not found, try second div (chapter-content)\n",
    "            content_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.chapter-content\")))\n",
    "\n",
    "        # Extract paragraphs inside the content div\n",
    "        paragraphs = content_div.find_elements(By.TAG_NAME, \"p\")\n",
    "        text = \"\\n\".join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "        return text if text else \"[Content not found]\"\n",
    "    except Exception as e:\n",
    "        return f\"[Error fetching content: {e}]\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#  Function to get all chapter links (with max page limit and repeat detection)\n",
    "def get_chapter_links(max_pages=100):\n",
    "    chapter_links = []\n",
    "    seen_urls = set()\n",
    "    page = 1\n",
    "    last_count = 0\n",
    "    repeat_pages = 0\n",
    "\n",
    "    while page <= max_pages:\n",
    "        url = f\"{BASE_URL}/{NOVEL_SLUG}.html?page={page}\"\n",
    "        print(f\"‚û°Ô∏è Fetching page {page}...\")\n",
    "        response = session.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        chapters = soup.select('ul.list-chapter li a')\n",
    "\n",
    "        if not chapters:\n",
    "            print(\"‚úÖ No chapters found on this page. Ending fetch.\")\n",
    "            break\n",
    "\n",
    "        new_links = 0\n",
    "        for chapter in chapters:\n",
    "            title = chapter.text.strip()\n",
    "            link = BASE_URL + chapter.get('href')\n",
    "            if link not in seen_urls:\n",
    "                seen_urls.add(link)\n",
    "                chapter_links.append((title, link))\n",
    "                new_links += 1\n",
    "\n",
    "        print(f\"üìÑ Found {new_links} new chapters on page {page}\")\n",
    "\n",
    "        # If nothing new was added, increase repeat counter\n",
    "        if len(chapter_links) == last_count:\n",
    "            repeat_pages += 1\n",
    "        else:\n",
    "            repeat_pages = 0\n",
    "\n",
    "        if repeat_pages >= 3:\n",
    "            print(\" Detected 3 repeated pages in a row. Stopping to avoid infinite loop.\")\n",
    "            break\n",
    "\n",
    "        last_count = len(chapter_links)\n",
    "        page += 1\n",
    "\n",
    "    return chapter_links[::1]\n",
    "\n",
    "# Save to DOCX file with progress bar\n",
    "def save_to_docx(chapters, start, end):\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"A Stay-at-home Dad‚Äôs Restaurant In An Alternate World\", 0)\n",
    "\n",
    "    for i in tqdm(range(start - 1, end), desc=\"Scraping chapters\", unit=\"chapter\"):\n",
    "        title, url = chapters[i]\n",
    "        print(f\"üìò Fetching: {title}\")\n",
    "        content = get_chapter_content(url)\n",
    "\n",
    "        doc.add_heading(f\"{title}\", level=1)\n",
    "        doc.add_paragraph(content)\n",
    "        time.sleep(1)  # Be respectful\n",
    "\n",
    "    filename = f\"Chapters_{start}_to_{end}.docx\"\n",
    "    doc.save(filename)\n",
    "    print(f\"\\n‚úÖ Done! Saved to {filename}\")\n",
    "\n",
    "#  Run scraper with input range\n",
    "print(\"Fetching chapter links...\")\n",
    "chapters = get_chapter_links(max_pages=100)\n",
    "total_chapters = len(chapters)\n",
    "print(f\"üìö Total chapters found: {total_chapters}\")\n",
    "\n",
    "try:\n",
    "    start = int(input(f\"Enter starting chapter number (1 to {total_chapters}): \"))\n",
    "    end = int(input(f\"Enter ending chapter number ({start} to {total_chapters}): \"))\n",
    "\n",
    "    if start < 1 or end > total_chapters or start > end:\n",
    "        raise ValueError(\"Invalid range.\")\n",
    "\n",
    "    save_to_docx(chapters, start, end)\n",
    "except ValueError as e:\n",
    "    print(\"‚ùå Error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
